{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MhoQ0WE77laV"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "_ckMIh7O7s6D"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Build TVM module, deploy on Tensorflow Serving and Infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbVhjPpzn6BM"
   },
   "source": [
    "This guide \n",
    "- Downloads Mobillenet model and builds using TVM frontend.\n",
    "- Deploy it on Tensorflow Serving\n",
    "- Perform inference through REST interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tvm\n",
    "from tvm import relay\n",
    "import os.path\n",
    "import tarfile,sys\n",
    "\n",
    "# Tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.framework import graph_pb2\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import tensor_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jAk1ZXqTJqN"
   },
   "source": [
    "## Download Mobilenet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MqDQO0KCaWS"
   },
   "outputs": [],
   "source": [
    "def untar(fname):\n",
    "    file_tar, file_tar_ext = os.path.splitext(fname)\n",
    "    print(file_tar)\n",
    "    if (fname.endswith(\"tgz\")):\n",
    "        tar = tarfile.open(fname)\n",
    "        tar.extractall(path=\"./\" + file_tar)\n",
    "        tar.close()\n",
    "        print(\"Extracted in Current Directory\")\n",
    "    else:\n",
    "        print(\"Not a tar.gz file\")\n",
    "\n",
    "\n",
    "def get_workload(path):\n",
    "    from mxnet.gluon.utils import download\n",
    "    download(path, \".\")\n",
    "\n",
    "    tar_name = os.path.basename(path)\n",
    "    untar(tar_name)\n",
    "\n",
    "    file_tar, file_tar_ext = os.path.splitext(tar_name)\n",
    "    model_name = file_tar + \"/\" + file_tar + \"_frozen.pb\"\n",
    "    return model_name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenet_v1_1.0_224\n",
      "Extracted in Current Directory\n"
     ]
    }
   ],
   "source": [
    "name = 'mobilenet_v1_1.0_224'\n",
    "dload_path ='http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/' + name +'.tgz'\n",
    "model_name = get_workload(dload_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PDu7OX8Nf5PY"
   },
   "source": [
    "## Compile the model on TVM and export the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LTNN0ANGgA36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-60f1446bb984>:14: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('conv2d', (1, 1, 1, 1024, 'float32'), (1, 1, 1024, 1001, 'float32'), (1, 1), (0, 0), (1, 1), 'NHWC', 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('conv2d', (1, 7, 7, 1024, 'float32'), (1, 1, 1024, 1024, 'float32'), (1, 1), (0, 0), (1, 1), 'NHWC', 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('conv2d', (1, 7, 7, 512, 'float32'), (1, 1, 512, 1024, 'float32'), (1, 1), (0, 0), (1, 1), 'NHWC', 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('conv2d', (1, 14, 14, 512, 'float32'), (1, 1, 512, 512, 'float32'), (1, 1), (0, 0), (1, 1), 'NHWC', 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('conv2d', (1, 14, 14, 256, 'float32'), (1, 1, 256, 512, 'float32'), (1, 1), (0, 0), (1, 1), 'NHWC', 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('conv2d', (1, 28, 28, 256, 'float32'), (1, 1, 256, 256, 'float32'), (1, 1), (0, 0), (1, 1), 'NHWC', 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('conv2d', (1, 28, 28, 128, 'float32'), (1, 1, 128, 256, 'float32'), (1, 1), (0, 0), (1, 1), 'NHWC', 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('conv2d', (1, 56, 56, 128, 'float32'), (1, 1, 128, 128, 'float32'), (1, 1), (0, 0), (1, 1), 'NHWC', 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('conv2d', (1, 56, 56, 64, 'float32'), (1, 1, 64, 128, 'float32'), (1, 1), (0, 0), (1, 1), 'NHWC', 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('conv2d', (1, 112, 112, 32, 'float32'), (1, 1, 32, 64, 'float32'), (1, 1), (0, 0), (1, 1), 'NHWC', 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=llvm, workload=('conv2d', (1, 225, 225, 3, 'float32'), (3, 3, 3, 32, 'float32'), (2, 2), (0, 0), (1, 1), 'NHWC', 'float32'). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    }
   ],
   "source": [
    "import tvm.relay.testing.tf as tf_testing\n",
    "\n",
    "def import_into_tvm(graph_def, input_data, input_node, num_output=1):\n",
    "    \"\"\" Generic function to compile on relay and execute on tvm \"\"\"\n",
    "\n",
    "    shape_dict = {input_node: input_data.shape}\n",
    "    dtype_dict = {input_node: input_data.dtype}\n",
    "\n",
    "    sym, params = relay.frontend.from_tensorflow(graph_def, layout=layout, shape=shape_dict)\n",
    "    return sym, params\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.gfile.FastGFile(os.path.join(\"./\", model_name), 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    graph = tf.import_graph_def(graph_def, name='')\n",
    "    # Call the utility to import the graph definition into default graph.\n",
    "    graph_def = tf_testing.ProcessGraphDefParam(graph_def)\n",
    "\n",
    "    in_shape = (1, 224, 224 , 3)    \n",
    "    shape_dict = {'input': in_shape}\n",
    "    dtype_dict = {'input': \"float32\"}\n",
    "\n",
    "    sym, params = relay.frontend.from_tensorflow(graph_def, shape=shape_dict)\n",
    "        \n",
    "    with relay.build_config(opt_level=3):\n",
    "        graph, lib, params = relay.build_module.build(\n",
    "            sym, target=\"llvm\", params=params)\n",
    "\n",
    "        lib.export_library(\"model.so\")\n",
    "        with open(\"model.json\", \"w\") as fo:\n",
    "            fo.write(graph)\n",
    "        with open(\"model.params\", \"wb\") as fo:\n",
    "            import nnvm\n",
    "            fo.write(nnvm.compiler.save_param_dict(params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwGPItyphqXT"
   },
   "source": [
    "## Deoply TVM Model on TF Serving\n",
    "\n",
    "You may use below template to deploy\n",
    "\n",
    "model_config_list: {\n",
    "  config: {\n",
    "    name: \"tvm_test\", # Some name of the model\n",
    "    base_path: \"path to model\", # In side this folder folder \"1\" contain model.json, model.so, model.params \n",
    "    model_platform: \"tvm\"\n",
    "  },\n",
    "}\n",
    "\n",
    "Finally run the serving using below command\n",
    "\n",
    "tensorflow_model_server --port=6003  --rest_api_port=6004 --model_config_file=/data/srk/Serving/models/model_config.txt\n",
    "\n",
    "The output should look like below\n",
    "\n",
    "\n",
    "2019-04-06 15:04:16.584957: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
    "2019-04-06 15:04:16.585035: I tensorflow_serving/model_servers/server_core.cc:559]  (Re-)adding model: tvm_test\n",
    "2019-04-06 15:04:16.685308: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: tvm_test version: 1}\n",
    "2019-04-06 15:04:16.685365: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: tvm_test version: 1}\n",
    "2019-04-06 15:04:16.685380: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: tvm_test version: 1}\n",
    "\n",
    "[15:04:17] tensorflow_serving/servables/tvm/tvm_loader.cc:73: Input:input\n",
    "[15:04:17] tensorflow_serving/servables/tvm/tvm_loader.cc:80: Output:InceptionV3/Predictions/Reshape_1\n",
    "2019-04-06 15:04:17.052067: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: tvm_test version: 1}\n",
    "2019-04-06 15:04:17.053701: I tensorflow_serving/model_servers/server.cc:331] Running gRPC ModelServer at 0.0.0.0:6003 ...\n",
    "[evhttp_server.cc : 237] RAW: Entering the event loop ...\n",
    "2019-04-06 15:04:17.054633: I tensorflow_serving/model_servers/server.cc:351] Exporting HTTP/REST API at:localhost:6004 ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "\"model_spec\":{\r\n",
      " \"name\": \"tvm\",\r\n",
      " \"signature_name\": \"\",\r\n",
      " \"version\": \"1\"\r\n",
      "}\r\n",
      ",\r\n",
      "\"metadata\": {\"signature_def\": {\r\n",
      " \"signature_def\": {\r\n",
      "  \"serving_default\": {\r\n",
      "   \"inputs\": {\r\n",
      "    \"input\": {\r\n",
      "     \"dtype\": \"DT_FLOAT\",\r\n",
      "     \"tensor_shape\": {\r\n",
      "      \"dim\": [\r\n",
      "       {\r\n",
      "        \"size\": \"1\",\r\n",
      "        \"name\": \"\"\r\n",
      "       },\r\n",
      "       {\r\n",
      "        \"size\": \"299\",\r\n",
      "        \"name\": \"\"\r\n",
      "       },\r\n",
      "       {\r\n",
      "        \"size\": \"299\",\r\n",
      "        \"name\": \"\"\r\n",
      "       },\r\n",
      "       {\r\n",
      "        \"size\": \"3\",\r\n",
      "        \"name\": \"\"\r\n",
      "       }\r\n",
      "      ],\r\n",
      "      \"unknown_rank\": false\r\n",
      "     },\r\n",
      "     \"name\": \"input\"\r\n",
      "    }\r\n",
      "   },\r\n",
      "   \"outputs\": {\r\n",
      "    \"InceptionV3/Predictions/Reshape_1\": {\r\n",
      "     \"dtype\": \"DT_FLOAT\",\r\n",
      "     \"tensor_shape\": {\r\n",
      "      \"dim\": [\r\n",
      "       {\r\n",
      "        \"size\": \"1\",\r\n",
      "        \"name\": \"\"\r\n",
      "       },\r\n",
      "       {\r\n",
      "        \"size\": \"1001\",\r\n",
      "        \"name\": \"\"\r\n",
      "       }\r\n",
      "      ],\r\n",
      "      \"unknown_rank\": false\r\n",
      "     },\r\n",
      "     \"name\": \"InceptionV3/Predictions/Reshape_1\"\r\n",
      "    }\r\n",
      "   },\r\n",
      "   \"method_name\": \"\"\r\n",
      "  }\r\n",
      " }\r\n",
      "}\r\n",
      "}\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "# We can see the model metata as below\n",
    "\n",
    "!curl http://localhost:6004/v1/models/tvm_test/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWg9X2QHlbGS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: {\"signature_name\": \"serving_default\", \"instances\": ... 33990479, 0.742600679397583, 0.7655767202377319]]]]}\n",
      "Json Resp: <Response [200]>\n",
      "Predictions: 1001\n"
     ]
    }
   ],
   "source": [
    "# Infer using RESt interface\n",
    "\n",
    "data = np.random.uniform(size=(1, 224, 224, 3)).astype('float32')\n",
    "\n",
    "import json\n",
    "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": data.tolist()})\n",
    "print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))\n",
    "\n",
    "import requests\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:6004/v1/models/tvm_test:predict', data=data, headers=headers)\n",
    "print(\"Json Resp:\", json_response)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "print(\"Predictions:\", len(predictions[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Serving_REST_simple.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
